import streamlit as st
import google.generativeai as genai

# ==========================================
# 1. ì„¤ì • ë° êµ¬ì„± (Configuration)
# ==========================================
st.set_page_config(
    page_title="ë‚˜ë§Œì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ ì…ë ¥ ë°›ê¸° (ë³´ì•ˆì„ ìœ„í•´)
# ì‹¤ì œ ë°°í¬ì‹œì—ëŠ” st.secretsë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
with st.sidebar:
    st.header("ì„¤ì • (Settings)")
    api_key = st.text_input("Google API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    
    # AI Studioì— ìˆë˜ 'System Instruction'ì„ ì—¬ê¸°ì— ë„£ìœ¼ì„¸ìš”
    system_instruction = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (System Instruction)",
        value="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
        height=200
    )
    
    st.divider()
    model_type = st.selectbox("ëª¨ë¸ ì„ íƒ", ["gemini-1.5-flash", "gemini-1.5-pro"])
    temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.0, 2.0, 1.0)

# ==========================================
# 2. ë¡œì§ êµ¬í˜„ (Logic)
# ==========================================

# API í‚¤ê°€ ì—†ìœ¼ë©´ ê²½ê³  í‘œì‹œ í›„ ì¤‘ë‹¨
if not api_key:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì— Google API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# Gemini ì„¤ì •
try:
    genai.configure(api_key=api_key)
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ëœ ëª¨ë¸ ìƒì„±
    model = genai.GenerativeModel(
        model_name=model_type,
        system_instruction=system_instruction,
        generation_config={"temperature": temperature}
    )
except Exception as e:
    st.error(f"API ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ê¸°ë¡ ì €ì¥ìš©)
if "messages" not in st.session_state:
    st.session_state.messages = []

# ==========================================
# 3. UI ë Œë”ë§ (UI Rendering)
# ==========================================

st.title("ğŸš€ My AI App Service")
st.caption("Powered by Google Gemini & Streamlit")

# ê¸°ì¡´ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€ (Context Retention)
            # GeminiëŠ” history ê°ì²´ë¥¼ ë”°ë¡œ ê´€ë¦¬í•˜ì§€ë§Œ, Streamlit ë°©ì‹ì— ë§ì¶° ë§¤ë²ˆ historyë¥¼ êµ¬ì„±í•˜ê±°ë‚˜
            # start_chatì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 1íšŒì„± í„´ ë°©ì‹ ì˜ˆì‹œì´ë‚˜,
            # ë©€í‹°í„´(ëŒ€í™” ê¸°ì–µ)ì„ ìœ„í•´ chat sessionì„ êµ¬ì„±í•©ë‹ˆë‹¤.
            
            history = [
                {"role": m["role"], "parts": [m["content"]]} 
                for m in st.session_state.messages[:-1] # í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì œì™¸
            ]
            
            chat = model.start_chat(history=history)
            response = chat.send_message(prompt, stream=True)
            
            # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ êµ¬í˜„
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            
        except Exception as e:
            st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
            full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3. AI ì‘ë‹µ ì €ì¥
    st.session_state.messages.append({"role": "model", "content": full_response})
