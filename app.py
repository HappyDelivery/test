import streamlit as st
import google.generativeai as genai
import os

# 1. í™”ë©´ ì„¤ì •
st.set_page_config(page_title="AI ì†”ë£¨ì…˜ ê°€ì´ë“œ", page_icon="ğŸ¤–")
st.title("ğŸ¤– AI ì†”ë£¨ì…˜ ê°€ì´ë“œ")

# 2. ë¹„ë°€ ê¸ˆê³ ì—ì„œ ì—¬ê¶Œ(API Key) êº¼ë‚´ê¸°
my_api_key = os.environ.get("GOOGLE_API_KEY")

if not my_api_key:
    st.info("ì„¤ì •ì„ ìœ„í•´ ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... (API Key ì¤€ë¹„ ì¤‘)")
    st.stop()

# 3. AI ë¡œë´‡ ì„¤ì •
genai.configure(api_key=my_api_key)

system_instruction = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ì¶° ìµœì ì˜ AI ë„êµ¬ë¥¼ ì¶”ì²œí•´ ì£¼ëŠ” 'AI í™œìš© ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´, ë‹¤ìŒ ìˆœì„œì™€ í˜•ì‹ì— ë§ì¶° ë‹µë³€í•´ì£¼ì„¸ìš”:
1. **ì¶”ì²œ AI ë„êµ¬:** (ê°€ì¥ ì í•©í•œ ë„êµ¬ ì´ë¦„)
2. **ê°€ê²© ì •ì±…:** (ë¬´ë£Œ / ìœ ë£Œ / ë¶€ë¶„ ìœ ë£Œ ë“± ëª…ì‹œ)
3. **í™œìš© ë°©ë²• (Step-by-Step):** ì´ˆë³´ìë„ ë”°ë¼ í•  ìˆ˜ ìˆê²Œ 1, 2, 3 ë‹¨ê³„ë¡œ ì•„ì£¼ ì‰½ê²Œ ì„¤ëª…
4. **í™œìš© ì˜ˆì‹œ:** ì‹¤ì œ ì ìš©í•´ ë³¼ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‚¬ë¡€
**ì£¼ì˜ì‚¬í•­:** ì„¤ëª…ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.
"""

# 4. [ì§€ëŠ¥í˜• ëª¨ë¸ ì—°ê²°] ì•Œì•„ì„œ ë˜ëŠ” ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤!
@st.cache_resource
def get_model():
    # ì‹œë„í•´ë³¼ ëª¨ë¸ ì´ë¦„ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
    candidates = [
        "gemini-1.5-flash-001",  # ê°€ì¥ ì•ˆì •ì ì¸ ë²„ì „ (1ìˆœìœ„)
        "gemini-1.5-flash",      # ê¸°ë³¸ ë³„ëª… (2ìˆœìœ„)
        "gemini-1.5-flash-002",  # ìµœì‹  ì—…ë°ì´íŠ¸ ë²„ì „ (3ìˆœìœ„)
        "gemini-pro"             # êµ¬í˜• ì•ˆì • ë²„ì „ (4ìˆœìœ„)
    ]
    
    selected_model = None
    for name in candidates:
        try:
            # í…ŒìŠ¤íŠ¸ ì—°ê²° ì‹œë„
            test_model = genai.GenerativeModel(name)
            # ì•„ì£¼ ê°„ë‹¨í•œ ì¸ì‚¬ë¡œ ìƒì¡´ í™•ì¸ (ë¹„ìš© ê±°ì˜ 0)
            test_model.generate_content("Hi")
            selected_model = name
            break # ì„±ê³µí•˜ë©´ ë°˜ë³µë¬¸ íƒˆì¶œ!
        except Exception:
            continue # ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ ëª¨ë¸ ì‹œë„

    if selected_model:
        return genai.GenerativeModel(selected_model, system_instruction=system_instruction), selected_model
    else:
        return None, None

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model, model_name = get_model()

if model:
    st.caption(f"ğŸš€ ì—°ê²° ì„±ê³µ! í˜„ì¬ ì‘ë™ ëª¨ë¸: {model_name}")
else:
    st.error("ğŸ˜­ ëª¨ë“  ëª¨ë¸ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API Key ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# 5. ì±„íŒ…ì°½ ë§Œë“¤ê¸°
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© ë³´ì—¬ì£¼ê¸°
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ìê°€ ì…ë ¥í–ˆì„ ë•Œ ì²˜ë¦¬
if prompt := st.chat_input("ì–´ë–¤ AIê°€ í•„ìš”í•˜ì‹ ê°€ìš”? (ì˜ˆ: ë¡œê³ ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ë¬´ë£Œ AI ì¶”ì²œí•´ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ì „ë¬¸ê°€ê°€ ë‹µë³€ì„ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                chat = model.start_chat(history=[]) 
                response = model.generate_content(prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"ë‹µë³€ ì¤‘ ì—ëŸ¬ê°€ ë‚¬ì–´ìš”: {e}")
