import streamlit as st
import google.generativeai as genai
import os

# 1. í™”ë©´ ì„¤ì •
st.set_page_config(page_title="AI ì†”ë£¨ì…˜ ê°€ì´ë“œ", page_icon="ğŸ¤–")
st.title("ğŸ¤– AI ì†”ë£¨ì…˜ ê°€ì´ë“œ")

# 2. ë¹„ë°€ ê¸ˆê³ ì—ì„œ ì—¬ê¶Œ(API Key) êº¼ë‚´ê¸°
my_api_key = os.environ.get("GOOGLE_API_KEY")

if not my_api_key:
    st.info("ì„¤ì •ì„ ìœ„í•´ ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... (API Key ì¤€ë¹„ ì¤‘)")
    st.stop()

# 3. AI ë¡œë´‡ ì„¤ì •
genai.configure(api_key=my_api_key)

system_instruction = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ì¶° ìµœì ì˜ AI ë„êµ¬ë¥¼ ì¶”ì²œí•´ ì£¼ëŠ” 'AI í™œìš© ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´, ë‹¤ìŒ ìˆœì„œì™€ í˜•ì‹ì— ë§ì¶° ë‹µë³€í•´ì£¼ì„¸ìš”:
1. **ì¶”ì²œ AI ë„êµ¬:** (ê°€ì¥ ì í•©í•œ ë„êµ¬ ì´ë¦„)
2. **ê°€ê²© ì •ì±…:** (ë¬´ë£Œ / ìœ ë£Œ / ë¶€ë¶„ ìœ ë£Œ ë“± ëª…ì‹œ)
3. **í™œìš© ë°©ë²• (Step-by-Step):** ì´ˆë³´ìë„ ë”°ë¼ í•  ìˆ˜ ìˆê²Œ 1, 2, 3 ë‹¨ê³„ë¡œ ì•„ì£¼ ì‰½ê²Œ ì„¤ëª…
4. **í™œìš© ì˜ˆì‹œ:** ì‹¤ì œ ì ìš©í•´ ë³¼ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‚¬ë¡€
**ì£¼ì˜ì‚¬í•­:** ì„¤ëª…ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.
"""

# 4. [ì§€ëŠ¥í˜• ëª¨ë¸ ì—°ê²°] ì„ ìƒë‹˜ ëª©ë¡ì— ìˆëŠ” 'ë˜ëŠ” ë†ˆ'ì„ ì°¾ìŠµë‹ˆë‹¤!
@st.cache_resource
def get_model():
    # â˜… í•µì‹¬ ìˆ˜ì •: ì„ ìƒë‹˜ íƒì • ëª©ë¡ì— 'ì‹¤ì œë¡œ ìˆë˜ ì´ë¦„'ë“¤ë§Œ ë„£ì—ˆìŠµë‹ˆë‹¤!
    candidates = [
        "gemini-2.0-flash-exp",   # 1ìˆœìœ„: ì„±ëŠ¥ ì¢‹ê³  ë¬´ë£Œ ì¿¼í„°ê°€ ë„‰ë„‰í•œ ì‹¤í—˜ ë²„ì „
        "gemini-flash-latest",    # 2ìˆœìœ„: ìµœì‹  í”Œë˜ì‹œ ëª¨ë¸ ë³„ëª…
        "gemini-2.0-flash-lite-preview-02-05", # 3ìˆœìœ„: ê°€ë²¼ìš´ ìµœì‹  í”„ë¦¬ë·°
        "gemini-exp-1206"         # 4ìˆœìœ„: ë˜ ë‹¤ë¥¸ ì‹¤í—˜ ë²„ì „
    ]
    
    selected_model = None
    last_error = None

    for name in candidates:
        try:
            # í…ŒìŠ¤íŠ¸ ì—°ê²° ì‹œë„
            test_model = genai.GenerativeModel(name)
            # ì•„ì£¼ ê°„ë‹¨í•œ ì¸ì‚¬ë¡œ ìƒì¡´ í™•ì¸
            test_model.generate_content("Hi")
            selected_model = name
            break # ì„±ê³µí•˜ë©´ ë°˜ë³µë¬¸ íƒˆì¶œ!
        except Exception as e:
            last_error = e
            continue # ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ ëª¨ë¸ ì‹œë„

    if selected_model:
        return genai.GenerativeModel(selected_model, system_instruction=system_instruction), selected_model, None
    else:
        return None, None, last_error

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model, model_name, error_msg = get_model()

if model:
    st.caption(f"ğŸš€ ì—°ê²° ì„±ê³µ! í˜„ì¬ ì‘ë™ ëª¨ë¸: {model_name}")
else:
    # ì–´ë–¤ ì—ëŸ¬ì¸ì§€ í™”ë©´ì— ìì„¸íˆ ë³´ì—¬ì¤ë‹ˆë‹¤ (ë””ë²„ê¹…ìš©)
    st.error(f"ğŸ˜­ ì—°ê²° ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\në§ˆì§€ë§‰ ì—ëŸ¬ ë‚´ìš©: {error_msg}")
    st.stop()

# 5. ì±„íŒ…ì°½ ë§Œë“¤ê¸°
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© ë³´ì—¬ì£¼ê¸°
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ìê°€ ì…ë ¥í–ˆì„ ë•Œ ì²˜ë¦¬
if prompt := st.chat_input("ì–´ë–¤ AIê°€ í•„ìš”í•˜ì‹ ê°€ìš”? (ì˜ˆ: ë¡œê³ ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ë¬´ë£Œ AI ì¶”ì²œí•´ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ì „ë¬¸ê°€ê°€ ë‹µë³€ì„ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                chat = model.start_chat(history=[]) 
                response = model.generate_content(prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"ë‹µë³€ ì¤‘ ì—ëŸ¬ê°€ ë‚¬ì–´ìš”: {e}")
